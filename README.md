# ğŸš¢ Titanic Survival Prediction - OOP Machine Learning Pipeline

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![Scikit-learn](https://img.shields.io/badge/scikit--learn-1.0+-orange.svg)](https://scikit-learn.org/)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)

ä¸€å€‹æ¡ç”¨ **ç‰©ä»¶å°å‘ç¨‹å¼è¨­è¨ˆ (OOP)** åŸå‰‡å»ºæ§‹çš„ç”Ÿç”¢ç´šæ©Ÿå™¨å­¸ç¿’æµç¨‹ï¼Œç”¨æ–¼é æ¸¬éµé”å°¼è™Ÿä¹˜å®¢å­˜æ´»ç‡ã€‚æœ¬å°ˆæ¡ˆå±•ç¤ºäº†å¦‚ä½•é‹ç”¨ SOLID åŸå‰‡ã€è¨­è¨ˆæ¨¡å¼å’Œæ¨¡çµ„åŒ–æ¶æ§‹ï¼Œæ‰“é€ å¯ç¶­è­·ã€å¯æ“´å±•çš„æ©Ÿå™¨å­¸ç¿’ç³»çµ±ã€‚

> ğŸ“Š **Kaggle Competition**: [Titanic - Machine Learning from Disaster](https://www.kaggle.com/competitions/titanic)

---

## ğŸ“‹ ç›®éŒ„

- [å°ˆæ¡ˆç‰¹è‰²](#å°ˆæ¡ˆç‰¹è‰²)
- [ç³»çµ±æ¶æ§‹](#ç³»çµ±æ¶æ§‹)
- [å¿«é€Ÿé–‹å§‹](#å¿«é€Ÿé–‹å§‹)
- [å°ˆæ¡ˆçµæ§‹](#å°ˆæ¡ˆçµæ§‹)
- [æ ¸å¿ƒçµ„ä»¶](#æ ¸å¿ƒçµ„ä»¶)
- [æ¨¡å‹æ•ˆèƒ½](#æ¨¡å‹æ•ˆèƒ½)
- [é€²éšç”¨æ³•](#é€²éšç”¨æ³•)
- [å¯¦é©—ç®¡ç†](#å¯¦é©—ç®¡ç†)
- [æ“´å±•æŒ‡å—](#æ“´å±•æŒ‡å—)
- [é–‹ç™¼æŒ‡å—](#é–‹ç™¼æŒ‡å—)
- [å¸¸è¦‹å•é¡Œ](#å¸¸è¦‹å•é¡Œ)
- [æˆæ¬Šè³‡è¨Š](#æˆæ¬Šè³‡è¨Š)
- [è¯çµ¡æ–¹å¼](#è¯çµ¡æ–¹å¼)

---

## âœ¨ å°ˆæ¡ˆç‰¹è‰²

### ğŸ—ï¸ æ¶æ§‹è¨­è¨ˆ
- **ç‰©ä»¶å°å‘è¨­è¨ˆ**: å®Œæ•´çš„ OOP æ¶æ§‹ï¼Œéµå¾ª SOLID åŸå‰‡
- **è¨­è¨ˆæ¨¡å¼æ‡‰ç”¨**: Factoryã€Strategyã€Pipelineã€Template Method
- **æ¨¡çµ„åŒ–çµæ§‹**: é«˜å…§èšä½è€¦åˆçš„çµ„ä»¶è¨­è¨ˆ
- **å¯æ“´å±•æ€§**: è¼•é¬†æ–°å¢æ¨¡å‹ã€å‰è™•ç†å™¨ã€ç‰¹å¾µå·¥ç¨‹å™¨

### ğŸ”§ åŠŸèƒ½ç‰¹æ€§
- **å¤šæ¨¡å‹æ”¯æ´**: Decision Treeã€Random Forestï¼ˆå¯è¼•é¬†æ“´å±•ï¼‰
- **è‡ªå‹•è¶…åƒæ•¸èª¿å„ª**: Grid Search / Random Search
- **å®Œæ•´è©•ä¼°æŒ‡æ¨™**: Accuracyã€Precisionã€Recallã€F1-Scoreã€ROC-AUC
- **å¯¦é©—è¿½è¹¤**: è‡ªå‹•è¨˜éŒ„æ¯æ¬¡è¨“ç·´çš„åƒæ•¸ã€æŒ‡æ¨™ã€ç”¢å‡º
- **è¦–è¦ºåŒ–**: æ±ºç­–æ¨¹åœ–ã€ç‰¹å¾µé‡è¦æ€§åœ–

### ğŸ“Š è³‡æ–™è™•ç†
- **æ™ºèƒ½å‰è™•ç†**: ç¼ºå¤±å€¼è™•ç†ã€ç•°å¸¸å€¼åµæ¸¬
- **è‡ªå‹•ç‰¹å¾µå·¥ç¨‹**: One-Hot Encodingã€ç‰¹å¾µé¸æ“‡
- **Pipeline æ©Ÿåˆ¶**: å¯çµ„åˆçš„è³‡æ–™è½‰æ›æµç¨‹

---

## ğŸ›ï¸ ç³»çµ±æ¶æ§‹

### æ•´é«”æ¶æ§‹åœ–

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Main Entry Point                       â”‚
â”‚                     (main.py)                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Pipeline Orchestration Layer                â”‚
â”‚                  (ml_pipeline.py)                        â”‚
â”‚  â€¢ æµç¨‹å”èª¿  â€¢ å¯¦é©—ç®¡ç†  â€¢ çµæœè¼¸å‡º                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚                 â”‚                 â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
    â–¼                â–¼  â–¼         â–¼  â–¼                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Data   â”‚   â”‚Preprocessâ”‚   â”‚ Feature  â”‚   â”‚  Model   â”‚
â”‚  Layer  â”‚   â”‚  Layer   â”‚   â”‚ Engineer â”‚   â”‚  Layer   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚              â”‚              â”‚               â”‚
    â–¼              â–¼              â–¼               â–¼
â€¢ DataLoader  â€¢ MissingValue  â€¢ OneHotEncoder â€¢ BaseModel
              â€¢ Outlier       â€¢ FamilySize    â€¢ DecisionTree
              â€¢ DropColumns                   â€¢ RandomForest
```

### è¨­è¨ˆæ¨¡å¼æ‡‰ç”¨

| æ¨¡å¼ | æ‡‰ç”¨ä½ç½® | èªªæ˜ |
|------|---------|------|
| **Factory Pattern** | [`ModelFactory`](core/models/model_factory.py) | çµ±ä¸€å‰µå»ºä¸åŒé¡å‹çš„æ¨¡å‹ |
| **Strategy Pattern** | [`BasePreprocessor`](core/preprocessing/preprocessor.py) | å¯åˆ‡æ›çš„å‰è™•ç†ç­–ç•¥ |
| **Pipeline Pattern** | [`MLPipeline`](core/pipeline/ml_pipeline.py) | ä¸²è¯è³‡æ–™è™•ç†æµç¨‹ |
| **Template Method** | [`BaseModel`](core/models/base_model.py) | å®šç¾©è¨“ç·´è©•ä¼°æµç¨‹éª¨æ¶ |

### SOLID åŸå‰‡é«”ç¾

- **S - å–®ä¸€è·è²¬**: æ¯å€‹é¡åˆ¥åªè² è²¬ä¸€é …åŠŸèƒ½
- **O - é–‹æ”¾å°é–‰**: å°æ“´å±•é–‹æ”¾ï¼ˆæ–°å¢æ¨¡å‹ï¼‰ï¼Œå°ä¿®æ”¹å°é–‰
- **L - é‡Œæ°æ›¿æ›**: æ‰€æœ‰æ¨¡å‹éƒ½å¯æ›¿æ› BaseModel
- **I - ä»‹é¢éš”é›¢**: æ¸…æ™°çš„æŠ½è±¡ä»‹é¢å®šç¾©
- **D - ä¾è³´åè½‰**: ä¾è³´æŠ½è±¡é¡åˆ¥è€Œéå…·é«”å¯¦ä½œ

---

## ğŸš€ å¿«é€Ÿé–‹å§‹

### ç’°å¢ƒéœ€æ±‚

```
Python 3.8+
pip 21.0+
```

### å®‰è£æ­¥é©Ÿ

1. **å…‹éš†å°ˆæ¡ˆ**

```bash
git clone https://github.com/yourusername/TitanicSurvivalClassifier.git
cd TitanicSurvivalClassifier
```

2. **å»ºç«‹è™›æ“¬ç’°å¢ƒ** (æ¨è–¦)

```bash
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
```

3. **å®‰è£ä¾è³´å¥—ä»¶**

```bash
pip install -r requirements.txt
```

### åŸºæœ¬ä½¿ç”¨

#### æ–¹æ³•ä¸€ï¼šä½¿ç”¨ main.py

```python
# ä¿®æ”¹ main.py ä¸­çš„è¨­å®š
MODEL_TYPE = "decision_tree"  # æˆ– "random_forest"
USE_TUNING = True
TUNING_METHOD = "grid"  # æˆ– "random"

# åŸ·è¡Œè¨“ç·´èˆ‡é æ¸¬
python main.py
```

#### æ–¹æ³•äºŒï¼šç¨‹å¼ç¢¼èª¿ç”¨

```python
from core.pipeline.ml_pipeline import MLPipeline

# å»ºç«‹ Pipeline
pipeline = MLPipeline(
    model_type="decision_tree",
    use_tuning=True,
    tuning_method="grid"
)

# è¨“ç·´æ¨¡å‹
pipeline.run_training_pipeline("data/raw/train.csv")

# åŸ·è¡Œæ¨è«–
pipeline.run_inference_pipeline(
    model_path="outputs/results_xxx/decision_tree_model.pkl",
    test_path="data/raw/test.csv"
)
```

### åŸ·è¡Œçµæœç¤ºä¾‹

```
============================================================
ğŸš€ é–‹å§‹è¨“ç·´ DECISION_TREE æ¨¡å‹
============================================================
ğŸ“Š å¯ç”¨æ¨¡å‹: ['decision_tree', 'random_forest']
âš™ï¸  è¶…åƒæ•¸èª¿å„ª: é–‹å•Ÿ
ğŸ” èª¿å„ªæ–¹æ³•: GRID
============================================================

ğŸ“ å»ºç«‹å¯¦é©—è³‡æ–™å¤¾: outputs/results_decision_tree_202511240913399760
ğŸ“Š ä½¿ç”¨æ¨¡å‹: DECISION_TREE

==================================================
é–‹å§‹è¶…åƒæ•¸èª¿å„ª (GRID Search)...
==================================================
Fitting 5 folds for each of 576 candidates, totalling 2880 fits

==================================================
è¶…åƒæ•¸èª¿å„ªå®Œæˆï¼
==================================================
æœ€ä½³äº¤å‰é©—è­‰åˆ†æ•¸: 0.8426

æœ€ä½³åƒæ•¸:
--------------------------------------------------
  criterion                : entropy
  max_depth                : 10
  max_features             : None
  min_impurity_decrease    : 0.0
  min_samples_leaf         : 5
  min_samples_split        : 2
==================================================

==================================================
æ¨¡å‹è©•ä¼°çµæœ (Model Evaluation Results)
==================================================
accuracy    : 0.7709
precision   : 0.7333
recall      : 0.6377
f1_score    : 0.6822
roc_auc     : 0.7803
==================================================

âœ… æ±ºç­–æ¨¹è¦–è¦ºåŒ–å·²å„²å­˜è‡³ outputs/results_xxx/decision_tree_visualization.png
âœ… ç‰¹å¾µé‡è¦æ€§åœ–å·²å„²å­˜è‡³ outputs/results_xxx/feature_importance.png

ç‰¹å¾µé‡è¦æ€§ (Top 10):
--------------------------------------------------
 1. Sex_female                      : 0.3734
 2. Fare                            : 0.2574
 3. Age                             : 0.1780
 4. Pclass_3                        : 0.1229
 5. SibSp                           : 0.0251
==================================================

âœ… é æ¸¬å®Œæˆï¼çµæœå·²å„²å­˜è‡³ outputs/results_xxx/submission.csv
```

---

## ğŸ“ å°ˆæ¡ˆçµæ§‹

```
TitanicSurvivalClassifier/
â”œâ”€â”€ core/                           # æ ¸å¿ƒç¨‹å¼åº«
â”‚   â”œâ”€â”€ data/                       # è³‡æ–™è¼‰å…¥æ¨¡çµ„
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ data_loader.py         # è³‡æ–™è®€å–å™¨
â”‚   â”‚
â”‚   â”œâ”€â”€ preprocessing/              # å‰è™•ç†æ¨¡çµ„
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ preprocessor.py        # å‰è™•ç†å™¨ï¼ˆç¼ºå¤±å€¼ã€ç•°å¸¸å€¼ç­‰ï¼‰
â”‚   â”‚
â”‚   â”œâ”€â”€ features/                   # ç‰¹å¾µå·¥ç¨‹æ¨¡çµ„
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ feature_engineer.py    # ç‰¹å¾µè½‰æ›å™¨
â”‚   â”‚
â”‚   â”œâ”€â”€ models/                     # æ¨¡å‹æ¨¡çµ„
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base_model.py          # æ¨¡å‹æŠ½è±¡åŸºåº•é¡åˆ¥
â”‚   â”‚   â”œâ”€â”€ decision_tree_classifier_model.py
â”‚   â”‚   â”œâ”€â”€ random_forest_model.py
â”‚   â”‚   â””â”€â”€ model_factory.py       # æ¨¡å‹å·¥å» 
â”‚   â”‚
â”‚   â””â”€â”€ pipeline/                   # æµç¨‹å”èª¿æ¨¡çµ„
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ ml_pipeline.py         # ä¸»è¦ Pipeline
â”‚
â”œâ”€â”€ data/                           # è³‡æ–™ç›®éŒ„
â”‚   â””â”€â”€ raw/
â”‚       â”œâ”€â”€ train.csv              # è¨“ç·´è³‡æ–™
â”‚       â””â”€â”€ test.csv               # æ¸¬è©¦è³‡æ–™
â”‚
â”œâ”€â”€ outputs/                        # è¼¸å‡ºç›®éŒ„
â”‚   â””â”€â”€ results_MODELTYPE_YYYYMMDDHHMMSS_xxx_N/
â”‚       â”œâ”€â”€ decision_tree_model.pkl         # è¨“ç·´å¥½çš„æ¨¡å‹
â”‚       â”œâ”€â”€ decision_tree_visualization.png # æ±ºç­–æ¨¹åœ–
â”‚       â”œâ”€â”€ feature_importance.png          # ç‰¹å¾µé‡è¦æ€§åœ–
â”‚       â”œâ”€â”€ experiment_report.txt           # å¯¦é©—å ±å‘Š
â”‚       â””â”€â”€ submission.csv                  # é æ¸¬çµæœ
â”‚
â”œâ”€â”€ main.py                         # ç¨‹å¼é€²å…¥é»
â”œâ”€â”€ requirements.txt                # ä¾è³´å¥—ä»¶æ¸…å–®
â”œâ”€â”€ README.md                       # å°ˆæ¡ˆèªªæ˜æ–‡ä»¶
â”œâ”€â”€ LICENSE                         # MIT æˆæ¬Š
â””â”€â”€ .gitignore                      # Git å¿½ç•¥æ¸…å–®
```

---

## ğŸ”§ æ ¸å¿ƒçµ„ä»¶

### 1. è³‡æ–™è¼‰å…¥å™¨ - [`DataLoader`](core/data/data_loader.py)

è² è²¬è®€å–å’Œé©—è­‰è³‡æ–™ã€‚

```python
from core.data.data_loader import DataLoader

loader = DataLoader()
df = loader.load_data("data/raw/train.csv")
```

### 2. å‰è™•ç†å™¨ - [`PreprocessingPipeline`](core/preprocessing/preprocessor.py)

æä¾›å¯çµ„åˆçš„å‰è™•ç†æ­¥é©Ÿã€‚

```python
from core.preprocessing.preprocessor import (
    PreprocessingPipeline,
    MissingValueHandler,
    DropColumnsPreprocessor
)

# å»ºç«‹å‰è™•ç†æµç¨‹
steps = [
    DropColumnsPreprocessor(
        columns_to_drop=["PassengerId", "Name", "Ticket", "Cabin"]
    ),
    MissingValueHandler(strategy="mean")
]

preprocessor = PreprocessingPipeline(steps=steps)
X_processed = preprocessor.fit_transform(X)
```

**å¯ç”¨çš„å‰è™•ç†å™¨ï¼š**

| é¡åˆ¥ | åŠŸèƒ½ | åƒæ•¸ |
|------|------|------|
| `MissingValueHandler` | è™•ç†ç¼ºå¤±å€¼ | `strategy`: "mean", "median", "mode", "drop" |
| `OutlierHandler` | è™•ç†ç•°å¸¸å€¼ | `method`: "zscore", "iqr" |
| `DropColumnsPreprocessor` | ç§»é™¤æ¬„ä½ | `columns_to_drop`: List[str] |

### 3. ç‰¹å¾µå·¥ç¨‹å™¨ - [`FeatureEngineerPipeline`](core/features/feature_engineer.py)

è‡ªå‹•åŒ–ç‰¹å¾µè½‰æ›ã€‚

```python
from core.features.feature_engineer import (
    FeatureEngineerPipeline,
    OneHotEncoder
)

steps = [
    OneHotEncoder(columns=["Sex", "Embarked", "Pclass"])
]

feature_engineer = FeatureEngineerPipeline(steps=steps)
X_features = feature_engineer.fit_transform(X)
```

### 4. æ¨¡å‹å±¤ - [`BaseModel`](core/models/base_model.py)

æ‰€æœ‰æ¨¡å‹çš„æŠ½è±¡åŸºåº•é¡åˆ¥ã€‚

```python
from abc import ABC, abstractmethod

class BaseModel(ABC):
    @abstractmethod
    def train(self, data):
        """è¨“ç·´æ¨¡å‹"""
        pass
    
    @abstractmethod
    def predict(self, input_data):
        """é€²è¡Œé æ¸¬"""
        pass
    
    @abstractmethod
    def evaluate(self, test_data):
        """è©•ä¼°æ¨¡å‹"""
        pass
    
    @abstractmethod
    def save_model(self, file_path):
        """å„²å­˜æ¨¡å‹"""
        pass
    
    @abstractmethod
    def load_model(self, file_path):
        """è¼‰å…¥æ¨¡å‹"""
        pass
```

### 5. æ¨¡å‹å·¥å»  - [`ModelFactory`](core/models/model_factory.py)

çµ±ä¸€çš„æ¨¡å‹å‰µå»ºä»‹é¢ã€‚

```python
from core.models.model_factory import ModelFactory

# å‰µå»ºæ±ºç­–æ¨¹æ¨¡å‹
model = ModelFactory.create_model(
    model_type="decision_tree",
    use_tuning=True,
    tuning_method="grid",
    cv=5
)

# å‰µå»ºéš¨æ©Ÿæ£®æ—æ¨¡å‹
model = ModelFactory.create_model(
    model_type="random_forest",
    use_tuning=True,
    tuning_method="random",
    cv=5
)
```

---

## ğŸ“Š æ¨¡å‹æ•ˆèƒ½

### Decision Tree (with Hyperparameter Tuning)

| æŒ‡æ¨™ | é©—è­‰é›†åˆ†æ•¸ | èªªæ˜ |
|------|-----------|------|
| **Accuracy** | 0.7709 | æ•´é«”é æ¸¬æº–ç¢ºç‡ |
| **Precision** | 0.7333 | æ­£é¡åˆ¥é æ¸¬ç²¾ç¢ºåº¦ |
| **Recall** | 0.6377 | æ­£é¡åˆ¥å¬å›ç‡ |
| **F1-Score** | 0.6822 | Precision èˆ‡ Recall çš„èª¿å’Œå¹³å‡ |
| **ROC-AUC** | 0.7803 | ROC æ›²ç·šä¸‹é¢ç© |

### æœ€ä½³è¶…åƒæ•¸

```python
{
    'criterion': 'entropy',
    'max_depth': 10,
    'max_features': None,
    'min_impurity_decrease': 0.0,
    'min_samples_leaf': 5,
    'min_samples_split': 2
}
```

### ç‰¹å¾µé‡è¦æ€§ (Top 5)

1. **Sex_female** (0.3734) - æ€§åˆ¥æ˜¯æœ€é‡è¦çš„å­˜æ´»é æ¸¬å› å­
2. **Fare** (0.2574) - èˆ¹ç¥¨åƒ¹æ ¼åæ˜ ç¤¾ç¶“åœ°ä½
3. **Age** (0.1780) - å¹´é½¡å½±éŸ¿å­˜æ´»ç‡
4. **Pclass_3** (0.1229) - ä¸‰ç­‰è‰™ä¹˜å®¢å­˜æ´»ç‡è¼ƒä½
5. **SibSp** (0.0251) - å…„å¼Ÿå§Šå¦¹/é…å¶æ•¸é‡

---

## ğŸ“ é€²éšç”¨æ³•

### æ–°å¢è‡ªè¨‚å‰è™•ç†å™¨

```python
from core.preprocessing.preprocessor import BasePreprocessor
from sklearn.preprocessing import StandardScaler
import pandas as pd

class CustomScaler(BasePreprocessor):
    """æ¨™æº–åŒ–æ•¸å€¼ç‰¹å¾µ"""
    
    def __init__(self, columns=None):
        self.columns = columns
        self.scaler = StandardScaler()
    
    def fit(self, X: pd.DataFrame):
        cols = self.columns or X.select_dtypes(include=['float64', 'int64']).columns
        self.scaler.fit(X[cols])
        return self
    
    def transform(self, X: pd.DataFrame):
        X = X.copy()
        cols = self.columns or X.select_dtypes(include=['float64', 'int64']).columns
        X[cols] = self.scaler.transform(X[cols])
        return X

# ä½¿ç”¨æ–¹å¼
from core.preprocessing.preprocessor import PreprocessingPipeline

steps = [
    DropColumnsPreprocessor(columns_to_drop=["PassengerId", "Name"]),
    CustomScaler(columns=["Age", "Fare"]),  # æ–°å¢çš„æ¨™æº–åŒ–å™¨
    MissingValueHandler(strategy="mean")
]

preprocessor = PreprocessingPipeline(steps=steps)
```

### æ–°å¢è‡ªè¨‚ç‰¹å¾µå·¥ç¨‹å™¨

```python
from core.features.feature_engineer import BaseFeatureEngineer
import pandas as pd

class FamilySizeFeature(BaseFeatureEngineer):
    """å‰µå»ºå®¶åº­äººæ•¸ç›¸é—œç‰¹å¾µ"""
    
    def fit(self, X: pd.DataFrame):
        return self
    
    def transform(self, X: pd.DataFrame):
        X = X.copy()
        # å®¶åº­ç¸½äººæ•¸ = è‡ªå·± + SibSp + Parch
        X['FamilySize'] = X['SibSp'] + X['Parch'] + 1
        # æ˜¯å¦ç¨è‡ªä¸€äºº
        X['IsAlone'] = (X['FamilySize'] == 1).astype(int)
        # å®¶åº­äººæ•¸åˆ†é¡
        X['FamilyCategory'] = pd.cut(
            X['FamilySize'],
            bins=[0, 1, 4, 20],
            labels=['Alone', 'Small', 'Large']
        )
        return X

# ä½¿ç”¨æ–¹å¼
from core.features.feature_engineer import FeatureEngineerPipeline

steps = [
    FamilySizeFeature(),  # æ–°å¢çš„ç‰¹å¾µå·¥ç¨‹å™¨
    OneHotEncoder(columns=["Sex", "Embarked", "FamilyCategory"])
]

feature_engineer = FeatureEngineerPipeline(steps=steps)
```

### æ–°å¢è‡ªè¨‚æ¨¡å‹

```python
from core.models.base_model import BaseModel
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
import joblib

class LogisticRegressionModel(BaseModel):
    """é‚è¼¯å›æ­¸æ¨¡å‹"""
    
    def __init__(self, use_tuning=True, tuning_method="grid", cv=5):
        self.use_tuning = use_tuning
        self.tuning_method = tuning_method
        self.cv = cv
        self.feature_names = None
        self.best_params = None
        
        self.default_params = {
            'C': 1.0,
            'penalty': 'l2',
            'solver': 'lbfgs',
            'max_iter': 200,
            'random_state': 42
        }
        
        self.param_grid = {
            'C': [0.001, 0.01, 0.1, 1, 10, 100],
            'penalty': ['l1', 'l2'],
            'solver': ['liblinear', 'saga']
        }
        
        self.model = LogisticRegression(**self.default_params)
    
    def train(self, data):
        X, y = data
        if hasattr(X, 'columns'):
            self.feature_names = X.columns.tolist()
        
        if self.use_tuning:
            search = GridSearchCV(
                estimator=LogisticRegression(random_state=42, max_iter=200),
                param_grid=self.param_grid,
                cv=self.cv,
                scoring='accuracy',
                n_jobs=-1
            )
            search.fit(X, y)
            self.model = search.best_estimator_
            self.best_params = search.best_params_
        else:
            self.model.fit(X, y)
    
    def predict(self, input_data):
        return self.model.predict(input_data)
    
    def evaluate(self, test_data):
        X_test, y_test = test_data
        y_pred = self.model.predict(X_test)
        y_pred_proba = self.model.predict_proba(X_test)[:, 1]
        
        return {
            'accuracy': accuracy_score(y_test, y_pred),
            'precision': precision_score(y_test, y_pred, zero_division=0),
            'recall': recall_score(y_test, y_pred, zero_division=0),
            'f1_score': f1_score(y_test, y_pred, zero_division=0),
            'roc_auc': roc_auc_score(y_test, y_pred_proba)
        }
    
    def save_model(self, file_path):
        model_data = {
            'model': self.model,
            'feature_names': self.feature_names,
            'best_params': self.best_params
        }
        joblib.dump(model_data, file_path)
    
    def load_model(self, file_path):
        model_data = joblib.load(file_path)
        self.model = model_data['model']
        self.feature_names = model_data.get('feature_names')
        self.best_params = model_data.get('best_params')

# è¨»å†Šåˆ°å·¥å» 
# ä¿®æ”¹ core/models/model_factory.py
models = {
    "decision_tree": DecisionTreeClassifierModel,
    "random_forest": RandomForestClassifierModel,
    "logistic_regression": LogisticRegressionModel,  # æ–°å¢
}
```

### æ‰¹é‡å¯¦é©—æ¯”è¼ƒ

```python
from core.pipeline.ml_pipeline import MLPipeline
import pandas as pd

def compare_models():
    """æ¯”è¼ƒä¸åŒæ¨¡å‹çš„æ•ˆèƒ½"""
    models = ["decision_tree", "random_forest"]
    results = []
    
    for model_type in models:
        print(f"\n{'='*60}")
        print(f"è¨“ç·´æ¨¡å‹: {model_type.upper()}")
        print(f"{'='*60}")
        
        # å»ºç«‹ Pipeline
        pipeline = MLPipeline(
            model_type=model_type,
            use_tuning=True,
            tuning_method="grid"
        )
        
        # è¨“ç·´ä¸¦å–å¾—çµæœ
        metrics = pipeline.run_training_pipeline("data/raw/train.csv")
        
        # è¨˜éŒ„çµæœ
        result = {'model': model_type}
        result.update(metrics)
        results.append(result)
    
    # å»ºç«‹æ¯”è¼ƒè¡¨
    df_results = pd.DataFrame(results)
    print("\n" + "="*60)
    print("æ¨¡å‹æ¯”è¼ƒçµæœ")
    print("="*60)
    print(df_results.to_string(index=False))
    
    # å„²å­˜æ¯”è¼ƒçµæœ
    df_results.to_csv("outputs/model_comparison.csv", index=False)
    print(f"\nâœ… æ¯”è¼ƒçµæœå·²å„²å­˜è‡³ outputs/model_comparison.csv")
    
    return df_results

if __name__ == "__main__":
    compare_models()
```

---

## ğŸ—‚ï¸ å¯¦é©—ç®¡ç†

### å¯¦é©—è³‡æ–™å¤¾çµæ§‹

æ¯æ¬¡åŸ·è¡Œéƒ½æœƒè‡ªå‹•å»ºç«‹ä¸€å€‹å”¯ä¸€çš„å¯¦é©—è³‡æ–™å¤¾ï¼š

```
outputs/
â””â”€â”€ results_MODELTYPE_YYYYMMDDHHMMSS_xxx_N/
    â”œâ”€â”€ decision_tree_model.pkl        # è¨“ç·´å¥½çš„æ¨¡å‹
    â”œâ”€â”€ decision_tree_visualization.png # æ±ºç­–æ¨¹è¦–è¦ºåŒ–
    â”œâ”€â”€ feature_importance.png          # ç‰¹å¾µé‡è¦æ€§åœ–
    â”œâ”€â”€ experiment_report.txt           # å®Œæ•´çš„å¯¦é©—å ±å‘Š
    â””â”€â”€ submission.csv                  # Kaggle æäº¤æª”æ¡ˆ
```

**è³‡æ–™å¤¾å‘½åè¦å‰‡:**
- `MODELTYPE`: æ¨¡å‹é¡å‹ï¼ˆdecision_tree, random_forestï¼‰
- `YYYYMMDDHHMMSS`: æ™‚é–“æˆ³è¨˜ï¼ˆå¹´æœˆæ—¥æ™‚åˆ†ç§’ï¼‰
- `xxx`: æ¯«ç§’
- `N`: æµæ°´åºè™Ÿï¼ˆåŒä¸€æ¯«ç§’å…§çš„ç¬¬ N æ¬¡åŸ·è¡Œï¼‰

### å¯¦é©—å ±å‘Šå…§å®¹

[`experiment_report.txt`](outputs/results_decision_tree_202511240913399760/experiment_report.txt) åŒ…å«ï¼š

```
============================================================
å¯¦é©—å ±å‘Š (Experiment Report)
============================================================

å¯¦é©—æ™‚é–“: 2025-11-24 09:13:45
å¯¦é©—è³‡æ–™å¤¾: outputs/results_decision_tree_202511240913399760
æ¨¡å‹é¡å‹: DECISION_TREE

------------------------------------------------------------
æ¨¡å‹è¨­å®š
------------------------------------------------------------
ä½¿ç”¨è¶…åƒæ•¸èª¿å„ª: True
èª¿å„ªæ–¹æ³•: grid
äº¤å‰é©—è­‰æŠ˜æ•¸: 5

------------------------------------------------------------
æœ€ä½³è¶…åƒæ•¸
------------------------------------------------------------
  criterion                     : entropy
  max_depth                     : 10
  max_features                  : None
  min_impurity_decrease         : 0.0
  min_samples_leaf              : 5
  min_samples_split             : 2

------------------------------------------------------------
æ¨¡å‹è©•ä¼°çµæœ
------------------------------------------------------------
  accuracy       : 0.7709
  precision      : 0.7333
  recall         : 0.6377
  f1_score       : 0.6822
  roc_auc        : 0.7803

------------------------------------------------------------
ç‰¹å¾µé‡è¦æ€§ (Top 10)
------------------------------------------------------------
   1. Sex_female                         : 0.3734
   2. Fare                               : 0.2574
   3. Age                                : 0.1780
   ...
============================================================
```

---

## ğŸ” æ“´å±•æŒ‡å—

### æ”¯æ´çš„æ¨¡å‹é¡å‹

ç›®å‰æ”¯æ´çš„æ¨¡å‹ï¼š

| æ¨¡å‹é¡å‹ | é¡åˆ¥åç¨± | æª”æ¡ˆä½ç½® |
|---------|---------|---------|
| Decision Tree | [`DecisionTreeClassifierModel`](core/models/decision_tree_classifier_model.py) | `core/models/decision_tree_classifier_model.py` |
| Random Forest | [`RandomForestModel`](core/models/random_forest_model.py) | `core/models/random_forest_model.py` |

**æ–°å¢æ¨¡å‹çš„æ­¥é©Ÿï¼š**

1. ç¹¼æ‰¿ [`BaseModel`](core/models/base_model.py)
2. å¯¦ä½œæ‰€æœ‰æŠ½è±¡æ–¹æ³•
3. åœ¨ [`ModelFactory`](core/models/model_factory.py) è¨»å†Šæ¨¡å‹
4. åœ¨ [`main.py`](main.py) ä¸­å³å¯ä½¿ç”¨

### åˆ‡æ›æ¨¡å‹

```python
# åœ¨ main.py ä¸­ä¿®æ”¹
MODEL_TYPE = "random_forest"  # æ”¹ç‚ºéš¨æ©Ÿæ£®æ—

# æˆ–åœ¨ç¨‹å¼ç¢¼ä¸­
pipeline = MLPipeline(model_type="random_forest")
```

### è¶…åƒæ•¸èª¿å„ªè¨­å®š

```python
# Grid Searchï¼ˆçª®èˆ‰æœå°‹ï¼Œæº–ç¢ºä½†æ…¢ï¼‰
pipeline = MLPipeline(
    model_type="decision_tree",
    use_tuning=True,
    tuning_method="grid"
)

# Random Searchï¼ˆéš¨æ©Ÿæœå°‹ï¼Œå¿«é€Ÿï¼‰
pipeline = MLPipeline(
    model_type="decision_tree",
    use_tuning=True,
    tuning_method="random"
)

# ä¸ä½¿ç”¨èª¿å„ªï¼ˆä½¿ç”¨é è¨­åƒæ•¸ï¼‰
pipeline = MLPipeline(
    model_type="decision_tree",
    use_tuning=False
)
```

---

## ğŸ› ï¸ é–‹ç™¼æŒ‡å—

### ä¾è³´å¥—ä»¶

```txt
pandas>=1.3.0
numpy>=1.21.0
scikit-learn>=1.0.0
joblib>=1.1.0
matplotlib>=3.5.0
```

### ç¨‹å¼ç¢¼é¢¨æ ¼

æœ¬å°ˆæ¡ˆéµå¾ª PEP 8 è¦ç¯„ï¼š

```bash
# æ ¼å¼åŒ–ç¨‹å¼ç¢¼
black core/ tests/

# æª¢æŸ¥ç¨‹å¼ç¢¼é¢¨æ ¼
pylint core/

# å‹åˆ¥æª¢æŸ¥
mypy core/
```

### æ¸¬è©¦

```bash
# åŸ·è¡Œæ‰€æœ‰æ¸¬è©¦
pytest tests/

# åŸ·è¡Œç‰¹å®šæ¸¬è©¦
pytest tests/test_models.py

# æŸ¥çœ‹æ¸¬è©¦è¦†è“‹ç‡
pytest --cov=core tests/
```

### è²¢ç»æŒ‡å—

æ­¡è¿è²¢ç»ï¼è«‹éµå¾ªä»¥ä¸‹æ­¥é©Ÿï¼š

1. Fork æœ¬å°ˆæ¡ˆ
2. å»ºç«‹åŠŸèƒ½åˆ†æ”¯ (`git checkout -b feature/AmazingFeature`)
3. æäº¤è®Šæ›´ (`git commit -m 'Add some AmazingFeature'`)
4. æ¨é€åˆ°åˆ†æ”¯ (`git push origin feature/AmazingFeature`)
5. é–‹å•Ÿ Pull Request

**ç¨‹å¼ç¢¼è¦æ±‚ï¼š**
- éµå¾ª PEP 8 è¦ç¯„
- æ‰€æœ‰ public æ–¹æ³•éƒ½éœ€è¦ docstring
- æ–°å¢åŠŸèƒ½éœ€åŒ…å«å–®å…ƒæ¸¬è©¦
- æ›´æ–°ç›¸é—œæ–‡ä»¶

---

## â“ å¸¸è¦‹å•é¡Œ

### Q1: å¦‚ä½•æŸ¥çœ‹æ‰€æœ‰å¯ç”¨çš„æ¨¡å‹ï¼Ÿ

```python
from core.models.model_factory import ModelFactory

available_models = ModelFactory.get_available_models()
print(available_models)  # ['decision_tree', 'random_forest']
```

### Q2: å¦‚ä½•èª¿æ•´ Decision Tree çš„æ·±åº¦é™åˆ¶ï¼Ÿ

ä¿®æ”¹ [`DecisionTreeClassifierModel`](core/models/decision_tree_classifier_model.py) ä¸­çš„ `param_grid`:

```python
self.param_grid = {
    "max_depth": [3, 5, 7, 10, 15, 20, None],  # æ–°å¢æ›´å¤šé¸é …
    # ...
}
```

### Q3: å¦‚ä½•ä½¿ç”¨è‡ªå·±çš„è³‡æ–™é›†ï¼Ÿ

1. æº–å‚™ CSV æ ¼å¼è³‡æ–™ï¼ˆéœ€åŒ…å« `Survived` æ¬„ä½ï¼‰
2. ä¿®æ”¹ [`main.py`](main.py) ä¸­çš„è·¯å¾‘
3. ç¢ºä¿è³‡æ–™æ ¼å¼èˆ‡ Titanic è³‡æ–™é›†ç›¸å®¹

### Q4: å¯¦é©—è³‡æ–™å¤¾å¤ªå¤šï¼Œå¦‚ä½•ç®¡ç†ï¼Ÿ

å»ºè­°å®šæœŸæ¸…ç†èˆŠçš„å¯¦é©—è³‡æ–™å¤¾ï¼Œæˆ–ä½¿ç”¨è…³æœ¬è‡ªå‹•ç®¡ç†ï¼š

```python
import os
import shutil
from pathlib import Path
from datetime import datetime, timedelta

def cleanup_old_experiments(days=30):
    """åˆªé™¤è¶…é N å¤©çš„å¯¦é©—è³‡æ–™å¤¾"""
    outputs_dir = Path("outputs")
    cutoff_date = datetime.now() - timedelta(days=days)
    
    for folder in outputs_dir.glob("results_*"):
        # å¾è³‡æ–™å¤¾åç¨±è§£ææ—¥æœŸ
        timestamp = folder.name.split("_")[2][:14]
        folder_date = datetime.strptime(timestamp, "%Y%m%d%H%M%S")
        
        if folder_date < cutoff_date:
            shutil.rmtree(folder)
            print(f"å·²åˆªé™¤: {folder}")

cleanup_old_experiments(days=30)
```

### Q5: å¦‚ä½•æäº¤åˆ° Kaggleï¼Ÿ

1. æ‰¾åˆ°æœ€æ–°çš„å¯¦é©—è³‡æ–™å¤¾
2. ä¸Šå‚³ `submission.csv` åˆ° Kaggle
3. æŸ¥çœ‹æ’è¡Œæ¦œçµæœ

---

## ğŸ“Š æ•ˆèƒ½å„ªåŒ–å»ºè­°

### 1. åŠ é€Ÿè¶…åƒæ•¸æœå°‹

```python
# ä½¿ç”¨ Random Search ä»£æ›¿ Grid Search
pipeline = MLPipeline(
    model_type="random_forest",
    use_tuning=True,
    tuning_method="random"  # æ›´å¿«
)
```

### 2. æ¸›å°‘è¶…åƒæ•¸æœå°‹ç©ºé–“

```python
# åœ¨æ¨¡å‹é¡åˆ¥ä¸­èª¿æ•´
self.param_grid = {
    "n_estimators": [100, 200],  # æ¸›å°‘é¸é …
    "max_depth": [10, 15, 20]    # æ¸›å°‘é¸é …
}
```

### 3. ä½¿ç”¨å¹³è¡Œé‹ç®—

å¤§å¤šæ•¸æ¨¡å‹å·²é è¨­å•Ÿç”¨ `n_jobs=-1`ï¼Œä½¿ç”¨æ‰€æœ‰ CPU æ ¸å¿ƒã€‚

---

## ğŸ—ºï¸ æœªä¾†è¦åŠƒ

- [ ] æ–°å¢æ›´å¤šæ¨¡å‹ï¼ˆXGBoostã€LightGBMã€CatBoostï¼‰
- [ ] å¯¦ä½œ Ensemble æ–¹æ³•ï¼ˆVotingã€Stackingï¼‰
- [ ] æ•´åˆ Optuna é€²è¡Œæ›´æ™ºèƒ½çš„è¶…åƒæ•¸èª¿å„ª
- [ ] åŠ å…¥ SHAP å€¼åˆ†ææ¨¡å‹å¯è§£é‡‹æ€§
- [ ] å»ºç«‹ Streamlit Web ä»‹é¢
- [ ] æ•´åˆ MLflow é€²è¡Œå¯¦é©—è¿½è¹¤
- [ ] å¯¦ä½œè‡ªå‹•ç‰¹å¾µé¸æ“‡
- [ ] æ–°å¢äº¤å‰é©—è­‰è¦–è¦ºåŒ–
- [ ] Docker å®¹å™¨åŒ–éƒ¨ç½²

---

## ğŸ“„ æˆæ¬Šè³‡è¨Š

æœ¬å°ˆæ¡ˆæ¡ç”¨ MIT License æˆæ¬Šã€‚

```
MIT License

Copyright (c) 2025 Chih-Chien Hsieh

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:
...
```

å®Œæ•´æˆæ¬Šå…§å®¹è«‹è¦‹ [LICENSE](LICENSE) æª”æ¡ˆã€‚

---

## ğŸ“§ è¯çµ¡æ–¹å¼

**ä½œè€…**: Chih-Chien Hsieh

- ğŸ“§ Email: twcch1218 [at] gmail.com
- ğŸ™ GitHub: [@twcch](https://github.com/twcch)

---

## ğŸ™ è‡´è¬

- [Kaggle Titanic Competition](https://www.kaggle.com/competitions/titanic) - æä¾›è³‡æ–™é›†å’Œç«¶è³½å¹³å°
- [Scikit-learn](https://scikit-learn.org/) - å¼·å¤§çš„æ©Ÿå™¨å­¸ç¿’å·¥å…·
- [Python Software Foundation](https://www.python.org/) - Python ç¨‹å¼èªè¨€
- æ‰€æœ‰é–‹æºè²¢ç»è€…

---

## ğŸ“š åƒè€ƒè³‡æº

### å®˜æ–¹æ–‡ä»¶
- [Scikit-learn Documentation](https://scikit-learn.org/stable/documentation.html)
- [Pandas Documentation](https://pandas.pydata.org/docs/)
- [Matplotlib Documentation](https://matplotlib.org/stable/contents.html)

### ç›¸é—œæ–‡ç« 
- [Design Patterns in Python](https://refactoring.guru/design-patterns/python)
- [Machine Learning Pipeline Best Practices](https://towardsdatascience.com/)
- [SOLID Principles in Python](https://realpython.com/solid-principles-python/)

### Kaggle è³‡æº
- [Titanic Competition Overview](https://www.kaggle.com/competitions/titanic)
- [Top Solutions](https://www.kaggle.com/competitions/titanic/discussion)
- [Feature Engineering Ideas](https://www.kaggle.com/competitions/titanic/data)

---

<div align="center">

**â­ å¦‚æœé€™å€‹å°ˆæ¡ˆå°ä½ æœ‰å¹«åŠ©ï¼Œè«‹çµ¦å€‹ Starï¼**

Made with â¤ï¸ by Chih-Chien Hsieh

</div>